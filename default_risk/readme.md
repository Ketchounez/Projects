# Проект: Прогнозирование дефолта по кредиту

## Цель проекта
Разработать модель машинного обучения для предсказания вероятности дефолта клиента на основе различных финансовых данных.

## Данные
В проекте используются данные, содержащие информацию о клиентах Home Credit, их кредитных историях, взаимодействии с банком и других характеристиках. Основные файлы:
- `application_train.csv` – обучающий набор данных с характеристиками клиентов.
- `application_test.csv` – тестовый набор данных.
- `bureau.csv` и `bureau_balance.csv` – информация о предыдущих кредитах клиентов в других финансовых организациях.
- `POS_CASH_balance.csv`, `credit_card_balance.csv`, `previous_application.csv` – данные о предыдущих кредитах, платежах по картам и поведении клиентов.
- `installments_payments.csv` – информация о выплатах по займам.

## Ход работы
1. **Предобработка данных**:
   - Объединение информации из разных таблиц.
   - Обработка пропусков (импутация медианными значениями).
   - Кодирование категориальных признаков (One-Hot Encoding, Label Encoding).
   - Нормализация числовых данных.

2. **Формирование нового набора признаков**:
   - Вычисление статистик по предыдущим кредитам.
   - Создание агрегированных показателей по платежам, задолженностям и кредитам.
   - Генерация **полиномиальных признаков** и анализ их корреляции с целевой переменной.
   - Исключение нерелевантных и слабокоррелированных признаков.

3. **Обучение моделей**:
   - Бейзлайн модели:
     -  `LightGBM`
     -  `CatBoost`
     -  `XGBoost`
     -  `LogisticRegression`
     -  `RandomForest`
   - Подбор гиперпараметров с помощью **Optuna** для поиска оптимальных значений.
   - Анализ качества различных моделей, формирование ансамблей. (Blending, Stacking)
   - Анализ важности признаков (Feature Importance) на основе лучшей модели (на основе показателя прироста информации gain).

4. **Оценка результатов**:
   - Использование метрики **ROC-AUC** для оценки качества моделей.
   - Валидация моделей на кросс-валидации.
   - Оптимизация модели на основе **feature engineering**.

## Результат
Финальная ансамблевая модель Blending (LightGBM + XGBoost) с **подобранными гиперпараметрами и улучшенными признаками** достигла **ROC-AUC = 0.7775**, при лучшем решении на Kaggle в **0.805**.

![Image](https://github.com/user-attachments/assets/08f9b2c2-47e4-4741-86ed-343d5c619c4f)
